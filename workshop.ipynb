{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG System Locally with Ollama, LlamaIndex, and Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0 - Install Workshop Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the workshop, ensure all necessary dependencies are installed in your Python environment. Use the following steps to set up your environment.\n",
    "\n",
    "### Step 1: Create a Virtual Environment\n",
    "\n",
    "Create and activate a virtual environment to isolate the workshop dependencies. For this workshop, we use **Python 3.11**. Choose between **venv** or **conda** (using Mamba for efficiency).\n",
    "\n",
    "##### Using `venv`\n",
    "\n",
    "On Linux/Mac:\n",
    "  ```bash\n",
    "  python3.11 -m venv local-rag\n",
    "  source local-rag/bin/activate\n",
    "  ```\n",
    "On Windows:\n",
    "  ```bash\n",
    "  python3.11 -m venv local-rag\n",
    "  local-rag\\Scripts\\activate\n",
    "  ```\n",
    "\n",
    "##### Using `conda`\n",
    "\n",
    "   ```bash\n",
    "   conda create -n local-rag python=3.11\n",
    "   conda activate local-rag\n",
    "   ```\n",
    "\n",
    "### Step 2: Install Required Packages\n",
    "\n",
    "Install all the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 3: Verify Installation\n",
    "\n",
    "Check that the key packages are installed correctly by importing them in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rhita.mamou/Downloads/lauzhack-workshop-2024-main/',\n",
       " '/Users/mamourhita/opt/anaconda3/envs/py311/lib/python311.zip',\n",
       " '/Users/mamourhita/opt/anaconda3/envs/py311/lib/python3.11',\n",
       " '/Users/mamourhita/opt/anaconda3/envs/py311/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/mamourhita/opt/anaconda3/envs/py311/lib/python3.11/site-packages',\n",
       " '/Users/mamourhita/Desktop/first-project/src']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/rhita.mamou/Downloads/lauzhack-workshop-2024-main/')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import llama_index\n",
    "import ollama\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Setting up Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Ollama\n",
    "\n",
    "First, download and install Ollama from the official website: [https://ollama.com/download/](https://ollama.com/download/).\n",
    "\n",
    "### Pull Required Models\n",
    "\n",
    "Open a terminal and run the following commands to download the necessary models:\n",
    "\n",
    "1. Pull the `llama3` model:\n",
    "   ```bash\n",
    "   ollama pull llama3\n",
    "   ```\n",
    "\n",
    "2. Pull the Nomic embedding model if required:\n",
    "   ```bash\n",
    "   ollama pull nomic\n",
    "   ```\n",
    "\n",
    "### Run the Model\n",
    "\n",
    "Once the models are installed, you can run the `llama3` model and test it by writing some prompts. Use the following command:\n",
    "\n",
    "```bash\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "Type a prompt and observe the output to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with Ollama in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPFL stands for École polytechnique fédérale de Lausanne, which translates to Polytechnic School of Lausanne in English. It is a Swiss federal university that was founded in 1858 and is now one of the top universities in Switzerland.\n",
      "\n",
      "EPFL is known for its strong programs in science, technology, engineering, and mathematics (STEM) fields, as well as humanities and social sciences. The university has a global reputation for excellence in research and innovation, particularly in areas such as physics, chemistry, computer science, engineering, and biology.\n",
      "\n",
      "EPFL has multiple campuses, with the main campus located in Lausanne, Switzerland. It also has a strong presence of international students from around the world, making it an attractive option for students looking to study abroad or pursue advanced research opportunities.\n",
      "\n",
      "The university is organized into several faculties, including:\n",
      "\n",
      "* Faculty of Arts and Social Sciences\n",
      "* Faculty of Biology and Medicine\n",
      "* Faculty of Engineering and Architecture\n",
      "* Faculty of Management, Law and Social Science\n",
      "\n",
      "EPFL has a strong industry connection and collaborative partnerships with companies such as IBM, Google, and Novartis, among others. The university is also ranked among the top universities globally in various rankings, including QS World University Rankings, Times Higher Education World University Rankings, and Academic Ranking of World Universities.\n",
      "\n",
      "Overall, EPFL is a leading institution for education, research, and innovation, and it plays an important role in shaping the future of science, technology, and society."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(model=\"llama3.2\", prompt=\"What is EPFL?\", stream=True)\n",
    "\n",
    "for r in response:\n",
    "    print(r[\"response\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Getting Started with LlamaIndex and ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LlamaIndex** ([official site](https://llamaindex.ai)) is a framework for connecting LLMs with data sources, enabling efficient retrieval and interaction with structured or unstructured data.\n",
    "\n",
    "**Chroma** ([official site](https://www.trychroma.com)) is a vector database designed for managing embeddings and serving as a retrieval layer for LLM applications.\n",
    "\n",
    "In this exercise, we’ll explore how to set up and use LlamaIndex to index and retrieve data in a **Chroma** database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Let's download a PDF\n",
    "\n",
    "You can start by adding documents to the `./docs` folder. If you don't know what to use, we suggest downloading the PDF at the following link:\n",
    "\n",
    "https://observationofalostsoul.wordpress.com/wp-content/uploads/2011/05/the-gospel-of-the-flying-spaghetti-monster.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Chroma as the Storage Backend\n",
    "\n",
    "Initialize the Chroma database and configure it for use with LlamaIndex. Here, we create an **Ephemeral Client** and collection, which stores data temporarily in memory without persisting it. This is ideal for testing and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"mydocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a **Persistent Client** that will preserve your database across sessions with:\n",
    "\n",
    "```python\n",
    "client = chromadb.PersistentClient(path=\"/path/to/save/to\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up LlamaIndex connectors\n",
    "\n",
    "Configure LlamaIndex to connect with Chroma as the vector store and set up a storage context. A **storage context** is an abstraction that manages how data is stored and retrieved, enabling seamless integration with different storage backends like Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load and explore documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use LlamaIndex's `SimpleDirectoryReader` to **ingest documents from a directory**. This utility reads files from a specified directory and prepares them for indexing by splitting the content into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '9c0813a6-bc50-4c77-85e6-85797d4b3c94',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_label': '1',\n",
       "  'file_name': '2379143-20_releve_de_postes_2024-04-17_00-55-17470.PDF',\n",
       "  'file_path': '/Users/mamourhita/Desktop/RAG/docs/2379143-20_releve_de_postes_2024-04-17_00-55-17470.PDF',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 101960,\n",
       "  'creation_date': '2024-11-17',\n",
       "  'last_modified_date': '2024-11-17'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'text': \"Compte privé CSX 2379143-20\\nMonnaie Francs suisses\\nIBAN CH86 0483 5237 9143 2000 0\\nVue d'ensemble du compte\\nSolde reporté\\nTotal des débits\\nTotal des crédits\\nSolde final\\n6.58\\n-1'779.61\\n2'240.00\\n466.97\\nCREDIT SUISSE (Suisse) SA\\nCH-8070 Zürich (0589)\\nCustomer Service Center 0848 880 842\\nClearing No. 4835   / BIC CRESCHZZ80A\\nMadame\\nRhita Mamou\\nChemin des Triaudes 4 Studio A302\\n1024 Ecublens VD\\nV\\n17 avril 2024\\nRelevé de postes détaillé 17.03.2024 au 16.04.2024\\nDate Texte Débit Crédit Valeur Solde\\n17.03.24 Solde  reporté 6.58\\n18.03.24 Paiement  clearing Donneur d'ordre:\\nRHITA MAMOU\\nCHAMBRE A 302 CH. DES TRIAUDES 4 CH\\n1024 ECUBLENS VD\\n50.00 18.03.24\\n18.03.24 Paiement  clearing Donneur d'ordre:\\nRHITA MAMOU\\nCHAMBRE A 302 CH. DES TRIAUDES 4 CH\\n1024 ECUBLENS VD\\n60.00 116.5818.03.24\\n19.03.24 TWINT  Paiement  du 18.03.24 à 22:46\\nEl Bouari, Nada +41788930164\\n18.70 19.03.24\\n19.03.24 TWINT  Paiement  du 18.03.24 à 15:20 LAVORENT\\n1207 Genève\\n30.00 67.8818.03.24\\n20.03.24 Carte  débit paiement point vente CHF\\nNestl. Enterprises S du 18.03.24 à 08:35\\nNo de carte 5574 88XX XXXX 9532\\n2.60 20.03.24\\n20.03.24 TWINT  Paiement  du 19.03.24 à 08:06\\nPadel Twint, Caterina +41775281039\\n13.00 19.03.24\\n20.03.24 Carte  débit paiement e-commerce ME\\nCHATGPT SUBSCRIPTION du 18.03.24 à 14:59\\nNo de carte 5574 88XX XXXX 9532\\nUSD 21.62 cours 0.903713 fixé le 19.03.24\\n19.55 32.7320.03.24\\n21.03.24 Carte  débit paiement point vente CHF\\nNestl. Enterprises S du 19.03.24 à 13:00\\nNo de carte 5574 88XX XXXX 9532\\n6.50 21.03.24\\n21.03.24 Paiement  clearing Donneur d'ordre:\\nRHITA MAMOU\\nCHAMBRE A 302 CH. DES TRIAUDES 4 CH\\n1024 ECUBLENS VD\\n50.00 76.2321.03.24\\n22.03.24 Carte  débit paiement point vente CHF\\nNestl. Enterprises S du 20.03.24 à 11:42\\nNo de carte 5574 88XX XXXX 9532\\n9.50 22.03.24\\nBE04\\nSuite page suivante\\nPage  1/4\",\n",
       " 'mimetype': 'text/plain',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n',\n",
       " 'class_name': 'Document'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the content of the documents further with a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "def data_to_df(nodes: List[TextNode]):\n",
    "    \"\"\"Convert a list of TextNode objects to a pandas DataFrame.\"\"\"\n",
    "    return pd.DataFrame([node.dict() for node in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_</th>\n",
       "      <th>embedding</th>\n",
       "      <th>metadata</th>\n",
       "      <th>excluded_embed_metadata_keys</th>\n",
       "      <th>excluded_llm_metadata_keys</th>\n",
       "      <th>relationships</th>\n",
       "      <th>text</th>\n",
       "      <th>mimetype</th>\n",
       "      <th>start_char_idx</th>\n",
       "      <th>end_char_idx</th>\n",
       "      <th>text_template</th>\n",
       "      <th>metadata_template</th>\n",
       "      <th>metadata_seperator</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9c0813a6-bc50-4c77-85e6-85797d4b3c94</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '1', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Compte privé CSX 2379143-20\\nMonnaie Francs su...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0eac77bd-868f-4fe0-96c6-8382485c8aff</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '2', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Relevé de postes détaillé 17.03.2024 au 16.04....</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37cd6a60-9927-4fde-a752-4ec94d531dc5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '3', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Relevé de postes détaillé 17.03.2024 au 16.04....</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7540ebe7-624a-49b8-8a03-6cd8bf0bbf70</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '4', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Relevé de postes détaillé 17.03.2024 au 16.04....</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a56c828-4bcf-4a60-922f-120e4c387113</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '1', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Compte privé CSX 2379143-20\\nMonnaie Francs su...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Document</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id_ embedding  \\\n",
       "0  9c0813a6-bc50-4c77-85e6-85797d4b3c94      None   \n",
       "1  0eac77bd-868f-4fe0-96c6-8382485c8aff      None   \n",
       "2  37cd6a60-9927-4fde-a752-4ec94d531dc5      None   \n",
       "3  7540ebe7-624a-49b8-8a03-6cd8bf0bbf70      None   \n",
       "4  1a56c828-4bcf-4a60-922f-120e4c387113      None   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'page_label': '1', 'file_name': '2379143-20_r...   \n",
       "1  {'page_label': '2', 'file_name': '2379143-20_r...   \n",
       "2  {'page_label': '3', 'file_name': '2379143-20_r...   \n",
       "3  {'page_label': '4', 'file_name': '2379143-20_r...   \n",
       "4  {'page_label': '1', 'file_name': '2379143-20_r...   \n",
       "\n",
       "                        excluded_embed_metadata_keys  \\\n",
       "0  [file_name, file_type, file_size, creation_dat...   \n",
       "1  [file_name, file_type, file_size, creation_dat...   \n",
       "2  [file_name, file_type, file_size, creation_dat...   \n",
       "3  [file_name, file_type, file_size, creation_dat...   \n",
       "4  [file_name, file_type, file_size, creation_dat...   \n",
       "\n",
       "                          excluded_llm_metadata_keys relationships  \\\n",
       "0  [file_name, file_type, file_size, creation_dat...            {}   \n",
       "1  [file_name, file_type, file_size, creation_dat...            {}   \n",
       "2  [file_name, file_type, file_size, creation_dat...            {}   \n",
       "3  [file_name, file_type, file_size, creation_dat...            {}   \n",
       "4  [file_name, file_type, file_size, creation_dat...            {}   \n",
       "\n",
       "                                                text    mimetype  \\\n",
       "0  Compte privé CSX 2379143-20\\nMonnaie Francs su...  text/plain   \n",
       "1  Relevé de postes détaillé 17.03.2024 au 16.04....  text/plain   \n",
       "2  Relevé de postes détaillé 17.03.2024 au 16.04....  text/plain   \n",
       "3  Relevé de postes détaillé 17.03.2024 au 16.04....  text/plain   \n",
       "4  Compte privé CSX 2379143-20\\nMonnaie Francs su...  text/plain   \n",
       "\n",
       "  start_char_idx end_char_idx                text_template metadata_template  \\\n",
       "0           None         None  {metadata_str}\\n\\n{content}    {key}: {value}   \n",
       "1           None         None  {metadata_str}\\n\\n{content}    {key}: {value}   \n",
       "2           None         None  {metadata_str}\\n\\n{content}    {key}: {value}   \n",
       "3           None         None  {metadata_str}\\n\\n{content}    {key}: {value}   \n",
       "4           None         None  {metadata_str}\\n\\n{content}    {key}: {value}   \n",
       "\n",
       "  metadata_seperator class_name  \n",
       "0                 \\n   Document  \n",
       "1                 \\n   Document  \n",
       "2                 \\n   Document  \n",
       "3                 \\n   Document  \n",
       "4                 \\n   Document  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df = data_to_df(documents)\n",
    "\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe several attributes, including `metadata`, `text`, `text_template`, and others. Let's focus on these three key categories:\n",
    "\n",
    "- **`metadata`**: This attribute contains additional information about the document, such as its source, creation date, or tags that can be used for filtering or retrieval purposes.\n",
    "- **`text`**: The main content of the document, representing the raw textual data that will be indexed and queried.\n",
    "- **`text_template`**: A structured format or schema for the document's text, often used to define how the content should be presented or processed during queries. \n",
    "\n",
    "These attributes play distinct roles in organizing and interacting with your data. Feel free to explore the different attributes at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Index and the documents\n",
    "\n",
    "To ingest documents into an index, we will need an embedder model to convert the document content into vector representations. These embeddings enable efficient similarity searches and retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamourhita/opt/anaconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/mamourhita/opt/anaconda3/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LlamaIndex, we can create an index using the `VectorStoreIndex` class, which enables efficient storage and retrieval of document embeddings and integrates with various storage backends and embedding models. We use here the chroma collection we previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 11/11 [00:00<00:00, 796.36it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:04<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Query the Index for Retrieval\n",
    "\n",
    "Once the documents are indexed, we can perform retrieval on them. This allows us to ask questions or search for relevant content based on the embeddings stored in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_</th>\n",
       "      <th>embedding</th>\n",
       "      <th>metadata</th>\n",
       "      <th>excluded_embed_metadata_keys</th>\n",
       "      <th>excluded_llm_metadata_keys</th>\n",
       "      <th>relationships</th>\n",
       "      <th>text</th>\n",
       "      <th>mimetype</th>\n",
       "      <th>start_char_idx</th>\n",
       "      <th>end_char_idx</th>\n",
       "      <th>text_template</th>\n",
       "      <th>metadata_template</th>\n",
       "      <th>metadata_seperator</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6384548-d6f7-4ee1-8032-71273bb332a8</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '1', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{'NodeRelationship.SOURCE': {'node_id': '9c081...</td>\n",
       "      <td>Compte privé CSX 2379143-20\\nMonnaie Francs su...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>1801</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>TextNode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1dca2c88-ab6d-4dc9-90ff-1f06f4a8f2e5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '5', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{'NodeRelationship.SOURCE': {'node_id': 'f6374...</td>\n",
       "      <td>10.24 TWINT  Paiement  du 12.10.24 à 09:22\\nSB...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>1540</td>\n",
       "      <td>2046</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>TextNode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffec4715-7a55-4e6b-a9ec-40a7eb882daf</td>\n",
       "      <td>None</td>\n",
       "      <td>{'page_label': '3', 'file_name': '2379143-20_r...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
       "      <td>{'NodeRelationship.SOURCE': {'node_id': '37cd6...</td>\n",
       "      <td>Relevé de postes détaillé 17.03.2024 au 16.04....</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>0</td>\n",
       "      <td>2027</td>\n",
       "      <td>{metadata_str}\\n\\n{content}</td>\n",
       "      <td>{key}: {value}</td>\n",
       "      <td>\\n</td>\n",
       "      <td>TextNode</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id_ embedding  \\\n",
       "0  c6384548-d6f7-4ee1-8032-71273bb332a8      None   \n",
       "1  1dca2c88-ab6d-4dc9-90ff-1f06f4a8f2e5      None   \n",
       "2  ffec4715-7a55-4e6b-a9ec-40a7eb882daf      None   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'page_label': '1', 'file_name': '2379143-20_r...   \n",
       "1  {'page_label': '5', 'file_name': '2379143-20_r...   \n",
       "2  {'page_label': '3', 'file_name': '2379143-20_r...   \n",
       "\n",
       "                        excluded_embed_metadata_keys  \\\n",
       "0  [file_name, file_type, file_size, creation_dat...   \n",
       "1  [file_name, file_type, file_size, creation_dat...   \n",
       "2  [file_name, file_type, file_size, creation_dat...   \n",
       "\n",
       "                          excluded_llm_metadata_keys  \\\n",
       "0  [file_name, file_type, file_size, creation_dat...   \n",
       "1  [file_name, file_type, file_size, creation_dat...   \n",
       "2  [file_name, file_type, file_size, creation_dat...   \n",
       "\n",
       "                                       relationships  \\\n",
       "0  {'NodeRelationship.SOURCE': {'node_id': '9c081...   \n",
       "1  {'NodeRelationship.SOURCE': {'node_id': 'f6374...   \n",
       "2  {'NodeRelationship.SOURCE': {'node_id': '37cd6...   \n",
       "\n",
       "                                                text    mimetype  \\\n",
       "0  Compte privé CSX 2379143-20\\nMonnaie Francs su...  text/plain   \n",
       "1  10.24 TWINT  Paiement  du 12.10.24 à 09:22\\nSB...  text/plain   \n",
       "2  Relevé de postes détaillé 17.03.2024 au 16.04....  text/plain   \n",
       "\n",
       "   start_char_idx  end_char_idx                text_template  \\\n",
       "0               0          1801  {metadata_str}\\n\\n{content}   \n",
       "1            1540          2046  {metadata_str}\\n\\n{content}   \n",
       "2               0          2027  {metadata_str}\\n\\n{content}   \n",
       "\n",
       "  metadata_template metadata_seperator class_name  \n",
       "0    {key}: {value}                 \\n   TextNode  \n",
       "1    {key}: {value}                 \\n   TextNode  \n",
       "2    {key}: {value}                 \\n   TextNode  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "nodes_with_score = retriever.retrieve(\"What is the Flying Spaghetti Monster?\")\n",
    "nodes = [n.node for n in nodes_with_score]\n",
    "data_to_df(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've retrieved your first data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Your First RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Retrieval-Augmented Generation (RAG) system, you need a Large Language Model (LLM) to generate answers to your queries by combining retrieved knowledge with the model's reasoning capabilities. At this point, Ollama comes to help as the LLM powering your RAG system. We set it up for LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready for querying your data. You can define a query engine and start asking it questions. Congrats, You have a working RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided transaction records for October 2024, the biggest category of expense appears to be 'Carte de débit paiement point vente CHF' (point-of-sale card purchases), with several transactions exceeding 7.00 CHF."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the biggest category of expense on Octobre 2024?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "LlamaIndex offers an easy way to improve the generated answer by prompting the LLM with a custom template, in which the relevant context will be fed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"Read carefully each of the bank report, analyse each expense, and try to categorize them\"\n",
    "    \n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similartiy_top_k=3,\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the biggest category of expense on Octobre 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with analyzing and categorizing expenses from a bank report. However, I don't see any specific bank report provided in your message.\n",
      "\n",
      "Please provide the actual bank report or the details of the expenses you'd like me to analyze, such as:\n",
      "\n",
      "* A list of transactions\n",
      "* A statement of accounts\n",
      "* Specific categories (e.g., housing, transportation, food)\n",
      "\n",
      "Once I have the necessary information, I'll do my best to categorize each expense and provide an analysis.\n",
      "\n",
      "Please paste the bank report or the expenses you'd like me to analyze, and I'll get started!"
     ]
    }
   ],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, a basic retriever is used. Let's look at how data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_score = response.source_nodes\n",
    "nodes = [n.node for n in nodes_with_score]\n",
    "data_to_df(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "node = nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do the models see exactly? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    node.get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    node.get_content(metadata_mode=MetadataMode.LLM),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to change the embeddings. For example, we can split the sentences in smaller blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index data\n",
    "index.vector_store.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter, SentenceWindowNodeParser\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=200)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    "    transformations=[sentence_splitter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many more ways to improve the RAG system, explore them on the official LlamaIndex page!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauzhack-workshop-2024-7gra-v0p-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
